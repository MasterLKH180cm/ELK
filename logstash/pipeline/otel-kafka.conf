input {
  kafka {
    bootstrap_servers => "${KAFKA_BROKERS:kafka:29092}"
    topics => ["otel-logs", "otel-traces", "otel-metrics"]
    group_id => "logstash-otel-unified-consumer"
    codec => json
    consumer_threads => 4
    poll_timeout_ms => 3000
    auto_offset_reset => "earliest"
    isolation_level => "read_committed"
    enable_auto_commit => true
    max_poll_records => 500
    session_timeout_ms => 30000
  }
}

filter {
  # Detect signal type (logs, traces, metrics) from OTLP structure
  ruby {
    code => '
      if event.get("resourceLogs")
        event.set("signal_type", "logs")
      elsif event.get("resourceSpans")
        event.set("signal_type", "traces")
      elsif event.get("resourceMetrics")
        event.set("signal_type", "metrics")
      else
        event.set("signal_type", "unknown")
      end
      event.set("source", "otel-collector")
    '
  }

  # OTLP JSON format: { "resourceLogs": [ { "resource": {...}, "scopeLogs": [ { "logRecords": [...] } ] } ] }
  
  # Parse resourceLogs if present
  if [resourceLogs] {
    ruby {
      code => '
        begin
          resource_logs = event.get("resourceLogs")
          return if !resource_logs || !resource_logs.is_a?(Array) || resource_logs.empty?
          
          resource_log = resource_logs[0]
          
          # Extract resource attributes
          resource_attrs = resource_log.dig("resource", "attributes") || []
          if resource_attrs.is_a?(Array)
            resource_attrs.each do |attr|
              key = attr["key"]
              value_obj = attr["value"]
              
              if key && value_obj.is_a?(Hash)
                actual_value = value_obj["stringValue"] || value_obj["intValue"] || value_obj["boolValue"] || value_obj["doubleValue"]
                
                case key
                when "service.name"
                  event.set("service_name", actual_value) if actual_value
                when "service.version"
                  event.set("service_version", actual_value) if actual_value
                when "service.instance.id"
                  event.set("service_instance_id", actual_value) if actual_value
                when "deployment.environment"
                  event.set("environment", actual_value) if actual_value
                when "host.name"
                  event.set("host_name", actual_value) if actual_value
                when "telemetry.sdk.language"
                  event.set("sdk_language", actual_value) if actual_value
                when "event.domain"
                  event.set("event_domain", actual_value) if actual_value
                end
              end
            end
          end
          
          # Find the first non-empty logRecords array in scopeLogs
          scope_logs = resource_log.dig("scopeLogs") || []
          log_record = nil
          
          if scope_logs.is_a?(Array)
            scope_logs.each do |scope|
              records = scope.dig("logRecords") || []
              if records.is_a?(Array) && !records.empty?
                log_record = records[0]
                break
              end
            end
          end
          
          return unless log_record
          
          # Extract log body
          log_body = log_record.dig("body")
          if log_body.is_a?(Hash)
            event.set("message", log_body["stringValue"])
          elsif log_body
            event.set("message", log_body.to_s)
          end
          
          # Extract severity
          severity_text = log_record["severityText"] || "UNSPECIFIED"
          event.set("log_level", severity_text)
          
          # Extract timestamp - Logstash expects @timestamp as an ISO8601 string
          time_unix_nano = log_record["observedTimeUnixNano"] || log_record["timeUnixNano"]
          if time_unix_nano && time_unix_nano.to_i > 0
            begin
              time_seconds = time_unix_nano.to_i / 1_000_000_000.0
              time_obj = Time.at(time_seconds).utc
              # Logstash accepts ISO8601 strings for @timestamp
              event.set("@timestamp", time_obj.iso8601(9))
            rescue => e
              event.set("timestamp_error", e.message)
            end
          end
          
          # Extract log record attributes
          log_attrs = log_record["attributes"] || []
          if log_attrs.is_a?(Array)
            log_attrs.each do |attr|
              key = attr["key"]
              value_obj = attr["value"]
              
              next unless key && value_obj.is_a?(Hash)
              
              # Extract event_domain from attributes if available
              if key == "event.domain"
                actual_val = value_obj["stringValue"]
                event.set("event_domain", actual_val) if actual_val
              end
              
              # Special handling for kvlistValue (extra_fields, etc)
              if value_obj["kvlistValue"]
                kv_list = value_obj["kvlistValue"]["values"] || []
                if kv_list.is_a?(Array)
                  kv_list.each do |kv|
                    kv_key = kv["key"]
                    kv_value = kv["value"]
                    if kv_key && kv_value.is_a?(Hash)
                      actual_val = kv_value["stringValue"] || kv_value["intValue"] || kv_value["boolValue"]
                      safe_kv_key = kv_key.gsub(".", "_")
                      
                      # Override service/environment fields from extra_fields (request headers take precedence)
                      case kv_key
                      when "service.name"
                        event.set("service_name", actual_val)
                      when "service.version"
                        event.set("service_version", actual_val)
                      when "deployment.environment"
                        event.set("environment", actual_val)
                      when "log.level"
                        event.set("log_level", actual_val)
                      when "event.type"
                        event.set("event_type", actual_val)
                      when "event.category"
                        event.set("event_category", actual_val)
                      when "event.domain"
                        event.set("event_domain", actual_val)
                      else
                        event.set(safe_kv_key, actual_val) if actual_val
                      end
                    end
                  end
                end
              else
                # Standard value extraction
                actual_value = value_obj["stringValue"] || value_obj["intValue"] || value_obj["boolValue"] || value_obj["doubleValue"]
                
                if actual_value
                  safe_key = key.gsub(".", "_")
                  event.set(safe_key, actual_value)
                end
              end
            end
          end
          
        rescue => e
          event.set("parse_error", e.message)
        end
      '
    }
  }

  # Skip empty messages
  if [message] == "" or ![message] {
    drop {}
  }

  # Clean up - remove nested/complex structures that may cause mapping issues
  mutate {
    remove_field => ["resourceLogs", "[@version]", "event", "scopeLogs", "timestamp_error"]
  }
}

output {
  # Route based on event_domain using data streams
  if [event_domain] == "auth" {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "auth"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  } else if [event_domain] == "frontend" {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "frontend"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  } else if [event_domain] == "backend" {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "backend"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  } else if [event_domain] == "security" {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "security"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  } else if [event_domain] == "logs" {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "logs"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  } else if [event_domain] == "traces" {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "traces"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  } else if [event_domain] == "metrics" {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "metrics"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  } else {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      data_stream_type => "logs"
      data_stream_dataset => "others"
      data_stream_namespace => "default"
      manage_template => false
      action => "create"
    }
  }

  # Debug output
  stdout {
    codec => json_lines
  }
}
