input {
  kafka {
    bootstrap_servers => "${KAFKA_BROKERS:kafka:29092}"
    topics => ["otel-logs"]
    group_id => "logstash-otel-consumer"
    codec => plain
    consumer_threads => 2
    poll_timeout_ms => 3000
    auto_offset_reset => "earliest"
    isolation_level => "read_committed"
  }
}

filter {
  # Parse JSON from Kafka message with error handling
  json {
    source => "message"
    target => "otel"
    tag_on_failure => ["json_parse_error"]
  }

  # Skip if JSON parsing failed
  if "json_parse_error" in [tags] {
    mutate {
      add_field => { "[@metadata][parse_status]" => "json_parse_failed" }
    }
  }

  # OTLP JSON format: { "resourceLogs": [ { "resource": {...}, "scopeLogs": [ { "logRecords": [...] } ] } ] }
  
  # Parse resourceLogs array only if JSON parsed successfully
  if [otel][resourceLogs] {
    ruby {
      code => '
        begin
          resource_logs = event.get("[otel][resourceLogs]")
          if resource_logs && resource_logs.is_a?(Array) && resource_logs.length > 0
            resource = resource_logs[0].dig("resource", "attributes") || {}
            scope_logs = resource_logs[0].dig("scopeLogs") || []
            
            if scope_logs.is_a?(Array) && scope_logs.length > 0
              log_records = scope_logs[0].dig("logRecords") || []
              
              if log_records.is_a?(Array) && log_records.length > 0
                log_record = log_records[0]
                
                # Extract log body
                event.set("message", log_record.dig("body", "stringValue") || log_record.dig("body") || "")
                
                # Extract severity
                severity_num = log_record.dig("severityNumber") || 0
                severity_text = log_record.dig("severityText") || "UNSPECIFIED"
                event.set("[log][level]", severity_text)
                
                # Extract timestamp
                time_unix_nano = log_record.dig("timeUnixNano") || log_record.dig("observedTimeUnixNano")
                if time_unix_nano
                  time_seconds = time_unix_nano.to_i / 1_000_000_000
                  time_nanos = time_unix_nano.to_i % 1_000_000_000
                  event.set("@timestamp", Time.at(time_seconds, time_nanos / 1000.0).iso8601(3))
                end
                
                # Extract attributes
                attributes = log_record.dig("attributes") || []
                if attributes.is_a?(Array)
                  attributes.each do |attr|
                    key = attr.dig("key")
                    value = attr.dig("value", "stringValue") || attr.dig("value")
                    event.set(key, value) if key && value
                  end
                end
                
                # Extract resource attributes
                if resource.is_a?(Hash)
                  resource.each do |key, val|
                    attr_value = val.dig("stringValue") || val
                    case key
                    when "service.name"
                      event.set("[service][name]", attr_value)
                    when "service.version"
                      event.set("[service][version]", attr_value)
                    when "deployment.environment"
                      event.set("[deployment][environment]", attr_value)
                    when "service.instance.id"
                      event.set("[service][instance][id]", attr_value)
                    when "host.name"
                      event.set("[host][name]", attr_value)
                    end
                  end
                end
              end
            end
          end
        rescue => e
          event.set("[@metadata][parse_error]", e.message)
        end
      '
    }
  }

  # Skip empty messages
  if [message] == "" or ![message] {
    drop {}
  }

  # Set default index metadata
  mutate {
    add_field => {
      "[@metadata][index_name]" => "otel-logs-%{+YYYY.MM.dd}"
    }
    # Only remove fields that exist
    remove_field => ["[@version]"]
  }

  # Remove otel field only if it exists
  if [otel] {
    mutate {
      remove_field => ["otel"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
    index => "%{[@metadata][index_name]}"
    manage_template => false
  }

  # Debug output (optional - comment out in production)
  stdout {
    codec => json_lines
  }
}
