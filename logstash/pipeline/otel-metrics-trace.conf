input {
  kafka {
    bootstrap_servers => "${KAFKA_BROKERS:kafka:29092}"
    topics_pattern => "^otel-(logs-.*|traces|metrics)$"
    group_id => "logstash-otel-unified-consumer"
    codec => json
    consumer_threads => 4
    poll_timeout_ms => 3000
    auto_offset_reset => "earliest"
    isolation_level => "read_committed"
    enable_auto_commit => true
    max_poll_records => 500
    session_timeout_ms => 30000
    decorate_events => true
  }
}

filter {
  # Detect signal type (logs, traces, metrics) from OTLP structure
  ruby {
    code => '
      if event.get("resourceLogs")
        event.set("signal_type", "logs")
      elsif event.get("resourceSpans")
        event.set("signal_type", "traces")
      elsif event.get("resourceMetrics")
        event.set("signal_type", "metrics")
      else
        event.set("signal_type", "unknown")
      end
      event.set("source", "otel-collector")
    '
  }

  # OTLP JSON format: { "resourceLogs": [ { "resource": {...}, "scopeLogs": [ { "logRecords": [...] } ] } ] }
  
  # Parse resourceLogs if present
  if [resourceLogs] {
    ruby {
      code => '
        begin
          resource_logs = event.get("resourceLogs")
          return if !resource_logs || !resource_logs.is_a?(Array) || resource_logs.empty?
          
          resource_log = resource_logs[0]
          
          # Extract resource attributes
          resource_attrs = resource_log.dig("resource", "attributes") || []
          if resource_attrs.is_a?(Array)
            resource_attrs.each do |attr|
              key = attr["key"]
              value_obj = attr["value"]
              
              if key && value_obj.is_a?(Hash)
                actual_value = value_obj["stringValue"] || value_obj["intValue"] || value_obj["boolValue"] || value_obj["doubleValue"]
                
                case key
                when "service.name"
                  event.set("service_name", actual_value) if actual_value
                when "service.version"
                  event.set("service_version", actual_value) if actual_value
                when "service.instance.id"
                  event.set("service_instance_id", actual_value) if actual_value
                when "deployment.environment"
                  event.set("environment", actual_value) if actual_value
                when "host.name"
                  event.set("host_name", actual_value) if actual_value
                when "telemetry.sdk.language"
                  event.set("sdk_language", actual_value) if actual_value
                when "event.domain"
                  event.set("event_domain", actual_value) if actual_value
                end
              end
            end
          end
          
          # Find the first non-empty logRecords array in scopeLogs
          scope_logs = resource_log.dig("scopeLogs") || []
          log_record = nil
          
          if scope_logs.is_a?(Array)
            scope_logs.each do |scope|
              records = scope.dig("logRecords") || []
              if records.is_a?(Array) && !records.empty?
                log_record = records[0]
                break
              end
            end
          end
          
          return unless log_record
          
          # Extract log body
          log_body = log_record.dig("body")
          msg_str = nil
          if log_body.is_a?(Hash)
            msg_str = log_body["stringValue"]
          elsif log_body
            msg_str = log_body.to_s
          end
          
          # Fallback: if message is empty or "{}", try azure.result.description
          if !msg_str || msg_str.empty? || msg_str == "{}"
            # Look for azure.result.description in resource attributes
             # Note: We already iterated resource_attrs above. We might have missed capturing it as "message".
             # Let\'s see if we captured it as "azure_result_description" in the event?
             # Actually, simpler: Check if event already has "azure_result_description" field set from above loop?
             # The loop above sets event fields dynamically.
             # Wait, the ruby code runs sequentially. The loop above handles resource attributes.
             # If "azure.result.description" was there, it would be set as event.set("azure_result_description", val).
             # BUT we need to access it here.
             # Let\'s access it from event directly.
             fallback = event.get("azure_result_description")
             msg_str = fallback if fallback
          end
          
          event.set("message", msg_str) if msg_str
          
          # Extract severity
          severity_text = log_record["severityText"] || "UNSPECIFIED"
          event.set("log_level", severity_text)
          
          # Extract timestamp - Logstash expects @timestamp as an ISO8601 string
          time_unix_nano = log_record["observedTimeUnixNano"] || log_record["timeUnixNano"]
          if time_unix_nano && time_unix_nano.to_i > 0
            begin
              time_seconds = time_unix_nano.to_i / 1_000_000_000.0
              time_obj = Time.at(time_seconds).utc
              # Logstash accepts ISO8601 strings for @timestamp
              event.set("@timestamp", time_obj.iso8601(9))
            rescue => e
              event.set("timestamp_error", e.message)
            end
          end
          
          # Extract log record attributes
          log_attrs = log_record["attributes"] || []
          if log_attrs.is_a?(Array)
            log_attrs.each do |attr|
              key = attr["key"]
              value_obj = attr["value"]
              
              next unless key && value_obj.is_a?(Hash)
              
              # Extract event_domain from attributes if available
              if key == "event.domain"
                actual_val = value_obj["stringValue"]
                event.set("event_domain", actual_val) if actual_val
              end
              
              # Special handling for kvlistValue (extra_fields, etc)
              if value_obj["kvlistValue"]
                kv_list = value_obj["kvlistValue"]["values"] || []
                if kv_list.is_a?(Array)
                  kv_list.each do |kv|
                    kv_key = kv["key"]
                    kv_value = kv["value"]
                    if kv_key && kv_value.is_a?(Hash)
                      actual_val = kv_value["stringValue"] || kv_value["intValue"] || kv_value["boolValue"]
                      safe_kv_key = kv_key.gsub(".", "_")
                      
                      # Override service/environment fields from extra_fields (request headers take precedence)
                      case kv_key
                      when "service.name"
                        event.set("service_name", actual_val)
                      when "service.version"
                        event.set("service_version", actual_val)
                      when "deployment.environment"
                        event.set("environment", actual_val)
                      when "log.level"
                        event.set("log_level", actual_val)
                      when "event.type"
                        event.set("event_type", actual_val)
                      when "event.category"
                        event.set("event_category", actual_val)
                      when "event.domain"
                        event.set("event_domain", actual_val)
                      else
                        event.set(safe_kv_key, actual_val) if actual_val
                      end
                    end
                  end
                end
              else
                # Standard value extraction
                actual_value = value_obj["stringValue"] || value_obj["intValue"] || value_obj["boolValue"] || value_obj["doubleValue"]
                
                if actual_value
                  safe_key = key.gsub(".", "_")
                  event.set(safe_key, actual_value)
                end
              end
            end
          end
          
        rescue => e
          event.set("parse_error", e.message)
        end
      '
    }
  }

  # Skip empty messages
  if [message] == "" or ![message] {
    drop {}
  }

  # Clean up - remove nested/complex structures that may cause mapping issues
  mutate {
    remove_field => ["resourceLogs", "[@version]", "event", "scopeLogs", "timestamp_error"]
  }

  # Dynamic Routing Logic (ported from logstash.conf)
  ruby {
    code => '
       domain = event.get("event.domain")
       
        # Fallback to topic pattern if not in attributes
        unless domain
          topic = event.get("[@metadata][kafka][topic]")
          event.set("debug_kafka_topic", topic) # DEBUG

          if topic && topic.match?(/^otel-logs-(.+)$/)
            match = topic.match(/^otel-logs-(.+)$/)[1]
            domain = match
            event.set("debug_routing", "matched_topic")
          elsif topic == "otel-traces"
            domain = "trace"
            event.set("debug_routing", "exact_trace")
          elsif topic == "otel-metrics"
            domain = "metrics"
            event.set("debug_routing", "exact_metrics")
          elsif topic == "fastapi-logs"
            domain = event.get("event_domain") || "logs"
            event.set("debug_routing", "fastapi_legacy")
          else
            domain = "default"
            event.set("debug_routing", "fallback_default_topic_mismatch")
          end
        end
       
       # Normalize
       domain = domain.downcase.gsub(/[^a-z0-9_-]/, "")
       event.set("event_domain", domain)
    '
  }
}

output {
  # Route based on event_domain using data streams
  # Route based on event_domain using data streams
  elasticsearch {
    hosts => ["${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
    # Use data streams!
    # Format: logs-{dataset}-{namespace}
    # We map "event_domain" to "dataset".
    data_stream => true
    data_stream_type => "logs"
    data_stream_dataset => "%{event_domain}" 
    data_stream_namespace => "data-stream"
    action => "create"
  }

  # Debug output
  stdout {
    codec => json_lines
  }
}
