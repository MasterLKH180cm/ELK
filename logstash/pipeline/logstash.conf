input {
  # Universal listener for all OTEL log topics
  kafka {
    bootstrap_servers => "${KAFKA_BROKERS:kafka:29092}"
    topics_pattern => "otel-logs-.*"
    group_id => "logstash-universal-consumer"
    codec => json
    consumer_threads => 4
    poll_timeout_ms => 3000
    auto_offset_reset => "earliest"
    # read_committed is safer but slightly slower; usually fine
    isolation_level => "read_committed" 
    enable_auto_commit => true
  }
}

filter {
  # 1. Parse timestamps
  if [body][attributes][time_unix_nano] {
     # If OTel provides nanoseconds, use it. But usually OTel JSON exporter might give body or separate fields.
     # The 'json' codec in input helps. OTLP JSON encoding usually has 'resourceLogs' etc if raw, but the 'otlp_json' encoding in collector
     # flattens it for easier consumption? Actually, 'encoding: otlp_json' produces the standard OTLP JSON structure.
     # However, Logstash might just see the record.
     # Let's assume standard Logstash OTel handling where possible.
     # But simplistic approach: use @timestamp if present, or time.now
  }

  if ![@timestamp] {
     ruby { code => 'event.set("@timestamp", LogStash::Timestamp.new(Time.now))' }
  }

  # 2. Extract Attributes to Root for easier indexing
  # 'otlp_json' exporter format puts attributes in specific places.
  # We'll pull them up.
  
  ruby {
    code => '
      # Helper to flatten OTel structure if needed
      # (This depends heavily on the OTel Collector exporter "otlp_json" structure, 
      # which often looks like { "resource": {...}, "scope": {...}, "logRecord": {...} } 
      # OR just the LogRecord if flattened by some other process.
      # But assuming standard "otlp_json" batch output:
      
      # NOTE: For simplicity, if we rely on a specific formatting, we might want "json" encoding in Collector instead of "otlp_json" 
      # if we want simple flat JSON. But "otlp_json" is standard.
      # Let"s just do a safe extract.
      
      # Extract Resource Attributes
      resource_attrs = event.get("[resource][attributes]")
      if resource_attrs.is_a?(Array)
        resource_attrs.each do |attr|
          event.set(attr["key"], attr["value"]["stringValue"] || attr["value"]["intValue"])
        end
      end
      
      # Extract Log Record Attributes (usually where our event.domain lives)
      # In OTLP JSON valid output, it"s often nested in resourceLogs -> scopeLogs -> logRecords
      # But Kafka exporter might flatten or send individual records? 
      # The "encoding: otlp_json" in collector sends the FULL OTLP JSON structure (Batch), 
      # so Logstash sees a ResourceLogs array. This is HARD to parse in Logstash without the "opentelemetry" codec.
      # HACK: If we use "encoding: json" (not otlp_json), we get a simplified structure? 
      # No, "otlp_json" is the only JSON encoding for Kafka exporter in some versions.
      # Actually, let"s trust the existing pattern. The previous file had [body] references.
      # We will try to find "event.domain" anywhere we can.
      
      domain = event.get("event.domain")
      
      # If not found at root, look in "attributes" map (common in some flat exporters)
      unless domain
        domain = event.get("[attributes][event.domain]")
      end
      
      # If still not found, try deep nested standard OTLP paths (Resource Attributes)
      # Scope Attributes...
      
      # Fallback to defaults based on topic if possible (meta info)
      unless domain
        topic = event.get("[@metadata][kafka][topic]")
        if topic && topic.include?("ohif")
           domain = "ohif"
        elsif topic && topic.include?("dictation-backend")
           domain = "dictation-backend"
        elsif topic && topic.include?("dictation-frontend")
           domain = "dictation-frontend"
        end
      end
      
      event.set("event_domain", domain || "default")
    '
  }

  # 3. Categorize Latency (if duration exists)
  if [event][duration_ms] {
    ruby {
      code => '
        d = event.get("[event][duration_ms]")
        if d.is_a?(Numeric)
           cat = d < 100 ? "fast" : (d < 500 ? "moderate" : "slow")
           event.set("latency_category", cat)
        end
      '
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    # Use data streams!
    # Format: logs-{dataset}-{namespace}
    # We map 'event_domain' to 'dataset'.
    # Ensure event_domain is safe for index names (lowercase, no spaces)
    data_stream => true
    data_stream_type => "logs"
    data_stream_dataset => "%{event_domain}" 
    data_stream_namespace => "default"
    action => "create"
  }
  
  # Debug to stdout for development
  # stdout { codec => rubydebug { metadata => true } }
}

