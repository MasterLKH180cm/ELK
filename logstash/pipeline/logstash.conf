input {
  # Universal listener for all OTEL log topics
  kafka {
    bootstrap_servers => "${KAFKA_BROKERS:kafka:29092}"
    topics_pattern => "otel-logs-.*"
    group_id => "logstash-universal-consumer"
    codec => json
    consumer_threads => 4
    poll_timeout_ms => 3000
    auto_offset_reset => "earliest"
    # read_committed is safer but slightly slower; usually fine
    isolation_level => "read_committed" 
    enable_auto_commit => true
    decorate_events => true
  }
}

filter {
  # 1. Split OTLP Batches
  # OTel 'otlp_json' sends a JSON object with "resourceLogs": [...]
  if [resourceLogs] {
    split {
      field => "[resourceLogs]"
    }
  }

  # 2. Extract Resource Attributes (Service Name, etc.)
  # These are at resourceLogs[].resource.attributes
  if [resourceLogs][resource][attributes] {
    ruby {
      code => '
        attrs = event.get("[resourceLogs][resource][attributes]")
        if attrs.is_a?(Array)
          attrs.each do |attr|
            # OTLP JSON attributes are { "key": "...", "value": { "stringValue": "..." } }
            key = attr["key"]
            val_obj = attr["value"]
            val = val_obj["stringValue"] || val_obj["intValue"] || val_obj["boolValue"] || val_obj["doubleValue"]
            event.set(key, val)
          end
        end
      '
    }
  }

  # 3. Split ScopeLogs (Instrumentation Scope)
  if [resourceLogs][scopeLogs] {
    split {
      field => "[resourceLogs][scopeLogs]"
    }
  }

  # 4. Split LogRecords
  if [resourceLogs][scopeLogs][logRecords] {
    split {
      field => "[resourceLogs][scopeLogs][logRecords]"
    }
  }

  # 5. Extract Log Attributes and Body
  if [resourceLogs][scopeLogs][logRecords] {
    ruby {
      code => '
        record = event.get("[resourceLogs][scopeLogs][logRecords]")
        
        # Move body to message
        if record["body"]
           body_val = record["body"]["stringValue"] || record["body"].to_s
           event.set("message", body_val)
        end
        
        # Extract Log Attributes
        attrs = record["attributes"]
        if attrs.is_a?(Array)
          attrs.each do |attr|
            key = attr["key"]
            val_obj = attr["value"]
            val = val_obj["stringValue"] || val_obj["intValue"] || val_obj["boolValue"] || val_obj["doubleValue"]
            event.set(key, val)
          end
        end
        
        # Use TimeUnixNano if available for @timestamp
        if record["timeUnixNano"]
           # nanoseconds to seconds string
           ts_nano = record["timeUnixNano"].to_s
           # Take first 10 digits for seconds, rest for micros/nanos
           # Ruby Logstash Timestamp expects Time object
           # Simplification: use current time or parse if critical.
           # event.set("@timestamp", LogStash::Timestamp.new(Time.at(ts_nano.to_f / 1000000000.0)))
        end
        
        # Clean up the huge object
        event.remove("[resourceLogs]")
      '
    }
  }

  # 6. Fallback Attribute Logic & Routing
  ruby {
    code => '
       domain = event.get("event.domain")
       
        # Fallback to topic pattern if not in attributes
        unless domain
          topic = event.get("[@metadata][kafka][topic]")
          event.set("debug_kafka_topic", topic) # DEBUG: Show us what Logstash sees!

          if topic && topic.match?(/^otel-logs-(.+)$/)
            match = topic.match(/^otel-logs-(.+)$/)[1]
            domain = match
            event.set("debug_routing", "matched_topic")
          elsif topic == "fastapi-logs"
            domain = event.get("event_domain") || "logs"
            event.set("debug_routing", "fastapi_legacy")
          else
            domain = "default"
            event.set("debug_routing", "fallback_default_topic_mismatch")
          end
        end
       
       # Normalize
       domain = domain.downcase.gsub(/[^a-z0-9_-]/, "")
       event.set("event_domain", domain)
    '
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    # Use data streams!
    # Format: logs-{dataset}-{namespace}
    # We map 'event_domain' to 'dataset'.
    # Ensure event_domain is safe for index names (lowercase, no spaces)
    data_stream => true
    data_stream_type => "logs"
    data_stream_dataset => "logs-%{event_domain}" 
    data_stream_namespace => "data-stream"
    action => "create"
  }
  
  # Debug to stdout for development
  stdout { codec => rubydebug { metadata => true } }
}

