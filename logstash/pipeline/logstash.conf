input {
  # Universal listener for all OTEL log topics
  kafka {
    bootstrap_servers => "${KAFKA_BROKERS:kafka:29092}"
    topics_pattern => "otel-logs-.*"
    group_id => "logstash-universal-consumer"
    codec => json
    consumer_threads => 4
    poll_timeout_ms => 3000
    auto_offset_reset => "earliest"
    # read_committed is safer but slightly slower; usually fine
    isolation_level => "read_committed" 
    enable_auto_commit => true
    decorate_events => true
  }
}

filter {
  # 1. Split OTLP Batches
  # OTel 'otlp_json' sends a JSON object with "resourceLogs": [...]
  if [resourceLogs] {
    split {
      field => "[resourceLogs]"
    }
  }

  # 2. Extract Resource Attributes (Service Name, etc.)
  # These are at resourceLogs[].resource.attributes
  if [resourceLogs][resource][attributes] {
    ruby {
      code => '
        attrs = event.get("[resourceLogs][resource][attributes]")
        if attrs.is_a?(Array)
          attrs.each do |attr|
            # OTLP JSON attributes are { "key": "...", "value": { "stringValue": "..." } }
            key = attr["key"]
            val_obj = attr["value"]
            val = val_obj["stringValue"] || val_obj["intValue"] || val_obj["boolValue"] || val_obj["doubleValue"]
            event.set(key, val)
          end
        end
      '
    }
  }

  # 3. Split ScopeLogs (Instrumentation Scope)
  if [resourceLogs][scopeLogs] {
    split {
      field => "[resourceLogs][scopeLogs]"
    }
  }

  # 4. Split LogRecords
  if [resourceLogs][scopeLogs][logRecords] {
    split {
      field => "[resourceLogs][scopeLogs][logRecords]"
    }
  }

  # 5. Extract Log Attributes and Body
  if [resourceLogs][scopeLogs][logRecords] {
    ruby {
      code => '
        record = event.get("[resourceLogs][scopeLogs][logRecords]")
        
        # Move body to message
        if record["body"]
           body_val = record["body"]["stringValue"] || record["body"].to_s
           event.set("message", body_val)
        end

        # Fallback: if message is empty or "{}", try azure.result.description
        msg_val = event.get("message")
        if !msg_val || msg_val.empty? || msg_val == "{}"
           # Check raw attribute for fallback (it might not be in event yet if we haven\'t iterated)
           # We iterate below, BUT we can look it up in record["attributes"] now.
           attrs = record["attributes"]
           if attrs.is_a?(Array)
             attrs.each do |attr|
               if attr["key"] == "azure.result.description"
                 val_obj = attr["value"]
                 fallback = val_obj["stringValue"]
                 event.set("message", fallback) if fallback
                 break
               end
             end
           end
        end
        
        # Extract Log Attributes
        attrs = record["attributes"]
        if attrs.is_a?(Array)
          attrs.each do |attr|
            key = attr["key"]
            val_obj = attr["value"]
            val = val_obj["stringValue"] || val_obj["intValue"] || val_obj["boolValue"] || val_obj["doubleValue"]
            event.set(key, val)
          end
        end
        
        # Use TimeUnixNano if available for @timestamp
        if record["timeUnixNano"]
           # nanoseconds to seconds string
           ts_nano = record["timeUnixNano"].to_s
           # Take first 10 digits for seconds, rest for micros/nanos
           # Ruby Logstash Timestamp expects Time object
           # Simplification: use current time or parse if critical.
           # event.set("@timestamp", LogStash::Timestamp.new(Time.at(ts_nano.to_f / 1000000000.0)))
        end
        
        # Clean up the huge object
        event.remove("[resourceLogs]")
      '
    }
  }

  # 6. Fallback Attribute Logic & Routing
  ruby {
    code => '
       # Try multiple possible field names for event domain
       # The transform processor sets attributes["event.domain"], which after OTLP JSON parsing
       # might be at different paths depending on how Logstash processes it
       domain = event.get("event.domain") || event.get("[event][domain]") || event.get("[event.domain]")
       
       # DEBUG: Show what we found
       event.set("debug_event_domain_found", domain || "nil")
       
       # Fallback to topic pattern if not in attributes
       unless domain
         topic = event.get("[@metadata][kafka][topic]")
         event.set("debug_kafka_topic", topic) # DEBUG: Show us what Logstash sees!

         if topic && topic.match?(/^otel-logs-(.+)$/)
           match = topic.match(/^otel-logs-(.+)$/)[1]
           domain = match
           event.set("debug_routing", "matched_topic: #{match}")
         elsif topic == "fastapi-logs"
           domain = event.get("event_domain") || "logs"
           event.set("debug_routing", "fastapi_legacy")
         else
           domain = "default"
           event.set("debug_routing", "fallback_default_topic_mismatch")
         end
       else
         event.set("debug_routing", "found_event_domain_attr")
       end
       
       # Normalize - keep hyphens and underscores
       domain = domain.to_s.downcase.gsub(/[^a-z0-9_-]/, "")
       event.set("[data_stream][dataset]", domain)
       event.set("debug_final_dataset", domain)
       
       # Also set event_domain for backward compatibility
       event.set("event_domain", domain)
       event.set("event.domain", domain)
    '
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    # Use data streams!
    # Format: logs-{dataset}-{namespace}
    # We set [data_stream][dataset] field in the filter
    data_stream => true
    data_stream_type => "logs"
    data_stream_namespace => "default"
    action => "create"
  }
  
  # Debug to stdout for development
  stdout { codec => rubydebug { metadata => true } }
}

