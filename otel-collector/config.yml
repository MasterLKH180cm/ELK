extensions:
  health_check:
    endpoint: "0.0.0.0:13133"

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

  # OHIF Logs
  azureeventhub/ohif:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_OHIF}
    format: "azure"
    # partition: "0" # Optional: set if needed for high throughput
    # offset: "@latest"

  # Dictation Backend Logs
  azureeventhub/dictation_backend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_BACKEND}
    format: "azure"

  # Dictation Frontend Logs
  azureeventhub/dictation_frontend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_FRONTEND}
    format: "azure"

processors:
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  resourcedetection:
    detectors: [env, system]
    override: false

  # Add attributes for routing and identification
  transform:
    log_statements:
      - context: log
        statements:
          - set(attributes["log.level"], severity_text)
          - set(attributes["event.domain"], resource.attributes["event.domain"]) where attributes["event.domain"] == nil and resource.attributes["event.domain"] != nil
          - set(attributes["event.domain"], "default") where attributes["event.domain"] == nil

          # Tag Azure sources based on scoping (receivers will tag via resource processors if possible, but here we can't easily distinguish receiver in transform unless we use separate pipelines with resource processors)
          # Better approach: Use resource processors in distinct pipelines. See 'service' section.

  # General attributes batching
  batch:
    send_batch_size: 256
    timeout: 2s
    send_batch_max_size: 1024

  # Resource processors for each pipeline to tag source
  resource/ohif:
    attributes:
      - key: event.domain
        value: "ohif"
        action: upsert
      - key: service.name
        value: "ohif-service"
        action: upsert

  resource/dictation_backend:
    attributes:
      - key: event.domain
        value: "dictation_backend"
        action: upsert
      - key: service.name
        value: "dictation_backend"
        action: upsert

  resource/dictation_frontend:
    attributes:
      - key: event.domain
        value: "dictation_frontend"
        action: upsert
      - key: service.name
        value: "dictation_frontend"
        action: upsert

  # Filter processors with strict matching using OTTL
  filter/auth:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "auth"'

  filter/session:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "session"'

  filter/worklist:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "worklist"'

  filter/viewer:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "viewer"'

  filter/trace:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "trace"'

  filter/metrics:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "metrics"'

  # Filter to exclude specific domains from default pipeline to prevent duplication
  filter/default_exclude:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "auth" and attributes["event.domain"] != "session" and attributes["event.domain"] != "worklist" and attributes["event.domain"] != "viewer" and attributes["event.domain"] != "trace" and attributes["event.domain"] != "metrics"'

exporters:
  debug:
    verbosity: basic

  kafka/traces:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-traces"
    topic: otel-traces
    encoding: otlp_json

  kafka/metrics:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-metrics"
    topic: otel-metrics
    encoding: otlp_json

  # Exporters for specific log domains
  kafka/logs-ohif:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-ohif"
    topic: otel-logs-ohif
    encoding: otlp_json

  kafka/logs-dictation_backend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-backend"
    topic: otel-logs-dictation_backend
    encoding: otlp_json

  kafka/logs-dictation_frontend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-frontend"
    topic: otel-logs-dictation_frontend
    encoding: otlp_json

  kafka/logs-default:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-default"
    topic: otel-logs-default
    encoding: otlp_json

  kafka/logs-auth:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-auth"
    topic: otel-logs-auth
    encoding: otlp_json

  kafka/logs-session:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-session"
    topic: otel-logs-session
    encoding: otlp_json

  kafka/logs-worklist:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-worklist"
    topic: otel-logs-worklist
    encoding: otlp_json

  kafka/logs-viewer:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-viewer"
    topic: otel-logs-viewer
    encoding: otlp_json

service:
  extensions: [health_check]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, batch]
      exporters: [debug, kafka/traces]

    metrics:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, batch]
      exporters: [debug, kafka/metrics]

    # Pipeline for OHIF (EventHub)
    logs/ohif:
      receivers: [azureeventhub/ohif]
      processors: [memory_limiter, resource/ohif, batch]
      exporters: [kafka/logs-ohif]

    # Pipeline for Dictation Backend (EventHub)
    logs/dictation_backend:
      receivers: [azureeventhub/dictation_backend]
      processors: [memory_limiter, resource/dictation_backend, batch]
      # Using shared dictation topic or could split further. Let's start with shared 'otel-logs-dictation'
      exporters: [kafka/logs-dictation_backend]

    # Pipeline for Dictation Frontend (EventHub)
    logs/dictation_frontend:
      receivers: [azureeventhub/dictation_frontend]
      processors: [memory_limiter, resource/dictation_frontend, batch]
      exporters: [kafka/logs-dictation_frontend]

    # Pipelines for Specific Domains (Filtered from OTLP)
    logs/auth:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/auth, batch]
      exporters: [kafka/logs-auth]

    logs/session:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/session, batch]
      exporters: [kafka/logs-session]

    logs/worklist:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/worklist, batch]
      exporters: [kafka/logs-worklist]

    logs/viewer:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/viewer, batch]
      exporters: [kafka/logs-viewer]

    # Pipeline for Default/Fallback Logs
    logs/default:
      receivers: [otlp]
      processors:
        [
          memory_limiter,
          resourcedetection,
          transform,
          filter/default_exclude,
          batch,
        ]
      exporters: [kafka/logs-default]
