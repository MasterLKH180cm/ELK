extensions:
  health_check:
    endpoint: "0.0.0.0:13133"

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

  # OHIF Logs
  azureeventhub/ohif:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_OHIF}
    format: "azure"
    # partition: "0" # Optional: set if needed for high throughput
    # offset: "@latest"

  # Dictation Backend Logs
  azureeventhub/dictation_backend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_BACKEND}
    format: "azure"

  # Dictation Frontend Logs
  azureeventhub/dictation_frontend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_FRONTEND}
    format: "azure"

processors:
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  resourcedetection:
    detectors: [env, system]
    override: false

  # Add attributes for routing and identification
  transform:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # ------------------------------------------------------------------
          # CRITICAL FIX: Handle the Map received from Fluent Bit
          # ------------------------------------------------------------------
          # 1. Merge the incoming Body (which is a Map) into Attributes
          - merge_maps(attributes, body, "upsert") 

          # 2. Now Attributes contains "event.domain", "service.namespace", etc.
          #    Promote them to Resource Attributes (for routing)
          - set(resource.attributes["event.domain"], attributes["event.domain"])
          - set(resource.attributes["service.namespace"], attributes["service.namespace"])
          - set(resource.attributes["app"], attributes["app"])
          - set(resource.attributes["deployment.environment"], attributes["deployment.environment"])

          # 3. Clean up: Set the Body to just the actual log message string
          - set(body, attributes["log"]) where attributes["log"] != nil
          
          # 4. Fallback (just in case)
          - set(resource.attributes["event.domain"], "default") where resource.attributes["event.domain"] == nil


# General attributes batching
  batch:
    send_batch_size: 256
    timeout: 2s
    send_batch_max_size: 1024

  # Resource processors for each pipeline to tag source
  resource/ohif:
    attributes:
      - key: event.domain
        value: "ohif"
        action: upsert
      - key: service.name
        value: "ohif-service"
        action: upsert

  resource/dictation_backend:
    attributes:
      - key: event.domain
        value: "dictation_backend"
        action: upsert
      - key: service.name
        value: "dictation_backend"
        action: upsert

  resource/dictation_frontend:
    attributes:
      - key: event.domain
        value: "dictation_frontend"
        action: upsert
      - key: service.name
        value: "dictation_frontend"
        action: upsert

  # Filter processors with strict matching using OTTL
  # IMPORTANT: In OTEL filter processor, TRUE = DROP, FALSE = KEEP
  # To KEEP auth-session logs, we DROP logs where event.domain != "auth-session"
  filter/auth-session:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "auth-session"'

  filter/worklist:
    error_mode: ignore
    logs:
      log_record:
        - ' attributes["event.domain"] != "worklist"'

  filter/viewer:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "viewer"'

  filter/trace:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "trace"'

  filter/metrics:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "metrics"'

  # Filter for default pipeline: DROP logs that belong to specific domains
  # This KEEPS only logs with event.domain NOT in the list above
  filter/default_exclude:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "auth-session" or attributes["event.domain"] == "worklist" or attributes["event.domain"] == "viewer" or attributes["event.domain"] == "trace" or attributes["event.domain"] == "metrics"'

exporters:
  debug:
    verbosity: detailed

  kafka/traces:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-traces"
    topic: otel-traces
    encoding: otlp_json

  kafka/metrics:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-metrics"
    topic: otel-metrics
    encoding: otlp_json

  # Exporters for specific log domains
  kafka/logs-ohif:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-ohif"
    topic: otel-logs-ohif
    encoding: otlp_json

  kafka/logs-dictation_backend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-backend"
    topic: otel-logs-dictation_backend
    encoding: otlp_json

  kafka/logs-dictation_frontend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-frontend"
    topic: otel-logs-dictation_frontend
    encoding: otlp_json

  kafka/logs-default:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-default"
    topic: otel-logs-default
    encoding: otlp_json

  kafka/logs-auth-session:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-auth-session"
    topic: otel-logs-auth-session
    encoding: otlp_json

  kafka/logs-worklist:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-worklist"
    topic: otel-logs-worklist
    encoding: otlp_json

  kafka/logs-viewer:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-viewer"
    topic: otel-logs-viewer
    encoding: otlp_json

service:
  extensions: [health_check]
  pipelines:
    # traces:
    #   receivers: [otlp]
    #   processors: [memory_limiter, resourcedetection, batch]
    #   exporters: [debug, kafka/traces]

    # metrics:
    #   receivers: [otlp]
    #   processors: [memory_limiter, resourcedetection, batch]
    #   exporters: [debug, kafka/metrics]

    # # Pipeline for OHIF (EventHub)
    # logs/ohif:
    #   receivers: [azureeventhub/ohif]
    #   processors: [memory_limiter, resource/ohif, batch]
    #   exporters: [kafka/logs-ohif]

    # # Pipeline for Dictation Backend (EventHub)
    # logs/dictation_backend:
    #   receivers: [azureeventhub/dictation_backend]
    #   processors: [memory_limiter, resource/dictation_backend, batch]
    #   # Using shared dictation topic or could split further. Let's start with shared 'otel-logs-dictation'
    #   exporters: [kafka/logs-dictation_backend]

    # # Pipeline for Dictation Frontend (EventHub)
    # logs/dictation_frontend:
    #   receivers: [azureeventhub/dictation_frontend]
    #   processors: [memory_limiter, resource/dictation_frontend, batch]
    #   exporters: [kafka/logs-dictation_frontend]

    # # Pipelines for Specific Domains (Filtered from OTLP)
    # logs/auth-session:
    #   receivers: [otlp]
    #   processors:
    #     [
    #       memory_limiter,
    #       resourcedetection,
    #       transform,
    #       filter/auth-session,
    #       batch,
    #     ]
    #   exporters: [debug, kafka/logs-auth-session]

    # logs/worklist:
    #   receivers: [otlp]
    #   processors:
    #     [memory_limiter, resourcedetection, transform, filter/worklist, batch]
    #   exporters: [kafka/logs-worklist]

    # logs/viewer:
    #   receivers: [otlp]
    #   processors:
    #     [memory_limiter, resourcedetection, transform, filter/viewer, batch]
    #   exporters: [kafka/logs-viewer]

    # # Pipeline for Default/Fallback Logs
    # logs/default:
    #   receivers: [otlp]
    #   processors:
    #     [
    #       memory_limiter,
    #       resourcedetection,
    #       transform,
    #       filter/default_exclude,
    #       batch,
    #     ]
    #   exporters: [debug, kafka/logs-default]
    logs/raw:
      receivers: [otlp]
      processors: []       # Skip processors here
      exporters: [debug]

    # Pipeline 2: Processed Data (With Transform)
    logs/processed:
      receivers: [otlp]
      processors: [transform]
      exporters: [debug]
