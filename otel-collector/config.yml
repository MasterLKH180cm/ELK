extensions:
  health_check:
    endpoint: "0.0.0.0:13133"

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

  # OHIF Logs
  azureeventhub/ohif:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_OHIF}
    format: "azure"
    # partition: "0" # Optional: set if needed for high throughput
    # offset: "@latest"

  # Dictation Backend Logs
  azureeventhub/dictation_backend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_BACKEND}
    format: "azure"

  # Dictation Frontend Logs
  azureeventhub/dictation_frontend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_FRONTEND}
    format: "azure"

processors:
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  resourcedetection:
    detectors: [env, system]
    override: false

  # Add attributes for routing and identification
  transform:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - set(attributes["log.level"], severity_text)

          # Case 1: Body is a JSON string (parse it first)
          # Use cache to store parsed JSON body
          - set(cache["parsed_body"], ParseJSON(body)) where IsString(body)

          # Case 2: Body is already a map (Fluent Bit OTLP direct)
          # Extract event.domain directly from body map
          - set(attributes["event.domain"], body["event.domain"]) where IsMap(body) and body["event.domain"] != nil

          # Extract event.domain from parsed JSON body (string case)
          - set(attributes["event.domain"], cache["parsed_body"]["event.domain"]) where attributes["event.domain"] == nil and cache["parsed_body"] != nil and cache["parsed_body"]["event.domain"] != nil

          # Then try resource attributes (for SDK-instrumented apps)
          - set(attributes["event.domain"], resource.attributes["event.domain"]) where attributes["event.domain"] == nil and resource.attributes["event.domain"] != nil

          # Extract other useful fields from body map (Case 2)
          - set(attributes["message"], body["log"]) where IsMap(body) and body["log"] != nil
          - set(attributes["container_name"], body["container_name"]) where IsMap(body) and body["container_name"] != nil
          - set(attributes["service.namespace"], body["service.namespace"]) where IsMap(body) and body["service.namespace"] != nil
          - set(attributes["deployment.environment"], body["deployment.environment"]) where IsMap(body) and body["deployment.environment"] != nil
          - set(attributes["app"], body["app"]) where IsMap(body) and body["app"] != nil

          # Extract other useful fields from parsed JSON body (Case 1)
          - set(attributes["message"], cache["parsed_body"]["log"]) where attributes["message"] == nil and cache["parsed_body"] != nil and cache["parsed_body"]["log"] != nil
          - set(attributes["container_name"], cache["parsed_body"]["container_name"]) where attributes["container_name"] == nil and cache["parsed_body"] != nil and cache["parsed_body"]["container_name"] != nil
          - set(attributes["service.namespace"], cache["parsed_body"]["service.namespace"]) where attributes["service.namespace"] == nil and cache["parsed_body"] != nil and cache["parsed_body"]["service.namespace"] != nil
          - set(attributes["deployment.environment"], cache["parsed_body"]["deployment.environment"]) where attributes["deployment.environment"] == nil and cache["parsed_body"] != nil and cache["parsed_body"]["deployment.environment"] != nil
          - set(attributes["app"], cache["parsed_body"]["app"]) where attributes["app"] == nil and cache["parsed_body"] != nil and cache["parsed_body"]["app"] != nil

          # Fallback to default if event.domain still not set
          - set(attributes["event.domain"], "default") where attributes["event.domain"] == nil

          # Tag Azure sources based on scoping (receivers will tag via resource processors if possible, but here we can't easily distinguish receiver in transform unless we use separate pipelines with resource processors)
          # Better approach: Use resource processors in distinct pipelines. See 'service' section.

  # General attributes batching
  batch:
    send_batch_size: 256
    timeout: 2s
    send_batch_max_size: 1024

  # Resource processors for each pipeline to tag source
  resource/ohif:
    attributes:
      - key: event.domain
        value: "ohif"
        action: upsert
      - key: service.name
        value: "ohif-service"
        action: upsert

  resource/dictation_backend:
    attributes:
      - key: event.domain
        value: "dictation_backend"
        action: upsert
      - key: service.name
        value: "dictation_backend"
        action: upsert

  resource/dictation_frontend:
    attributes:
      - key: event.domain
        value: "dictation_frontend"
        action: upsert
      - key: service.name
        value: "dictation_frontend"
        action: upsert

  # Filter processors with strict matching using OTTL
  # IMPORTANT: In OTEL filter processor, TRUE = DROP, FALSE = KEEP
  # To KEEP auth-session logs, we DROP logs where event.domain != "auth-session"
  filter/auth-session:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "auth-session"'

  filter/worklist:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "worklist"'

  filter/viewer:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "viewer"'

  filter/trace:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "trace"'

  filter/metrics:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "metrics"'

  # Filter for default pipeline: DROP logs that belong to specific domains
  # This KEEPS only logs with event.domain NOT in the list above
  filter/default_exclude:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "auth-session" or attributes["event.domain"] == "worklist" or attributes["event.domain"] == "viewer" or attributes["event.domain"] == "trace" or attributes["event.domain"] == "metrics"'

exporters:
  debug:
    verbosity: basic

  kafka/traces:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-traces"
    topic: otel-traces
    encoding: otlp_json

  kafka/metrics:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-metrics"
    topic: otel-metrics
    encoding: otlp_json

  # Exporters for specific log domains
  kafka/logs-ohif:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-ohif"
    topic: otel-logs-ohif
    encoding: otlp_json

  kafka/logs-dictation_backend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-backend"
    topic: otel-logs-dictation_backend
    encoding: otlp_json

  kafka/logs-dictation_frontend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-frontend"
    topic: otel-logs-dictation_frontend
    encoding: otlp_json

  kafka/logs-default:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-default"
    topic: otel-logs-default
    encoding: otlp_json

  kafka/logs-auth-session:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-auth-session"
    topic: otel-logs-auth-session
    encoding: otlp_json

  kafka/logs-worklist:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-worklist"
    topic: otel-logs-worklist
    encoding: otlp_json

  kafka/logs-viewer:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-viewer"
    topic: otel-logs-viewer
    encoding: otlp_json

service:
  extensions: [health_check]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, batch]
      exporters: [debug, kafka/traces]

    metrics:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, batch]
      exporters: [debug, kafka/metrics]

    # Pipeline for OHIF (EventHub)
    logs/ohif:
      receivers: [azureeventhub/ohif]
      processors: [memory_limiter, resource/ohif, batch]
      exporters: [kafka/logs-ohif]

    # Pipeline for Dictation Backend (EventHub)
    logs/dictation_backend:
      receivers: [azureeventhub/dictation_backend]
      processors: [memory_limiter, resource/dictation_backend, batch]
      # Using shared dictation topic or could split further. Let's start with shared 'otel-logs-dictation'
      exporters: [kafka/logs-dictation_backend]

    # Pipeline for Dictation Frontend (EventHub)
    logs/dictation_frontend:
      receivers: [azureeventhub/dictation_frontend]
      processors: [memory_limiter, resource/dictation_frontend, batch]
      exporters: [kafka/logs-dictation_frontend]

    # Pipelines for Specific Domains (Filtered from OTLP)
    logs/auth-session:
      receivers: [otlp]
      processors:
        [
          memory_limiter,
          resourcedetection,
          transform,
          filter/auth-session,
          batch,
        ]
      exporters: [kafka/logs-auth-session]

    logs/worklist:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/worklist, batch]
      exporters: [kafka/logs-worklist]

    logs/viewer:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/viewer, batch]
      exporters: [kafka/logs-viewer]

    # Pipeline for Default/Fallback Logs
    logs/default:
      receivers: [otlp]
      processors:
        [
          memory_limiter,
          resourcedetection,
          transform,
          filter/default_exclude,
          batch,
        ]
      exporters: [kafka/logs-default]
