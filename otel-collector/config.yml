extensions:
  health_check:
    endpoint: "0.0.0.0:13133"

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

  # OHIF Logs
  azureeventhub/ohif:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_OHIF}
    format: "azure"
    # partition: "0" # Optional: set if needed for high throughput
    # offset: "@latest"

  # Dictation Backend Logs
  azureeventhub/dictation_backend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_BACKEND}
    format: "azure"

  # Dictation Frontend Logs
  azureeventhub/dictation_frontend:
    connection: ${env:AZURE_EVENTHUB_CONNECTION_STRING_DICTATION_FRONTEND}
    format: "azure"

processors:
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  resourcedetection:
    detectors: [env, system]
    override: false

  # Add attributes for routing and identification
  transform:
  error_mode: ignore
  log_statements:
    - context: log
      statements:
        # Copy resource attributes to log attributes for routing
        - set(attributes["event.domain"], resource.attributes["event.domain"]) where resource.attributes["event.domain"] != nil
        - set(attributes["service.namespace"], resource.attributes["service.namespace"]) where resource.attributes["service.namespace"] != nil
        - set(attributes["deployment.environment"], resource.attributes["deployment.environment"]) where resource.attributes["deployment.environment"] != nil
        - set(attributes["app"], resource.attributes["app"]) where resource.attributes["app"] != nil

        # Set log level from severity
        - set(attributes["log.level"], severity_text) where severity_text != nil

        # Handle message field (Fluent Bit â†’ body.stringValue)
        - set(attributes["message"], body.stringValue) where body.stringValue != nil and attributes["message"] == nil

        # Fallback domain
        - set(attributes["event.domain"], "default") where attributes["event.domain"] == nil

  # General attributes batching
  batch:
    send_batch_size: 256
    timeout: 2s
    send_batch_max_size: 1024

  # Resource processors for each pipeline to tag source
  resource/ohif:
    attributes:
      - key: event.domain
        value: "ohif"
        action: upsert
      - key: service.name
        value: "ohif-service"
        action: upsert

  resource/dictation_backend:
    attributes:
      - key: event.domain
        value: "dictation_backend"
        action: upsert
      - key: service.name
        value: "dictation_backend"
        action: upsert

  resource/dictation_frontend:
    attributes:
      - key: event.domain
        value: "dictation_frontend"
        action: upsert
      - key: service.name
        value: "dictation_frontend"
        action: upsert

  # Filter processors with strict matching using OTTL
  # IMPORTANT: In OTEL filter processor, TRUE = DROP, FALSE = KEEP
  # To KEEP auth-session logs, we DROP logs where event.domain != "auth-session"
  filter/auth-session:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "auth-session"'

  filter/worklist:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "worklist"'

  filter/viewer:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "viewer"'

  filter/trace:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "trace"'

  filter/metrics:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] != "metrics"'

  # Filter for default pipeline: DROP logs that belong to specific domains
  # This KEEPS only logs with event.domain NOT in the list above
  filter/default_exclude:
    error_mode: ignore
    logs:
      log_record:
        - 'attributes["event.domain"] == "auth-session" or attributes["event.domain"] == "worklist" or attributes["event.domain"] == "viewer" or attributes["event.domain"] == "trace" or attributes["event.domain"] == "metrics"'

exporters:
  debug:
    verbosity: basic

  kafka/traces:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-traces"
    topic: otel-traces
    encoding: otlp_json

  kafka/metrics:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-metrics"
    topic: otel-metrics
    encoding: otlp_json

  # Exporters for specific log domains
  kafka/logs-ohif:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-ohif"
    topic: otel-logs-ohif
    encoding: otlp_json

  kafka/logs-dictation_backend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-backend"
    topic: otel-logs-dictation_backend
    encoding: otlp_json

  kafka/logs-dictation_frontend:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-dictation-frontend"
    topic: otel-logs-dictation_frontend
    encoding: otlp_json

  kafka/logs-default:
    brokers:
      - kafka:29092
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-default"
    topic: otel-logs-default
    encoding: otlp_json

  kafka/logs-auth-session:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-auth-session"
    topic: otel-logs-auth-session
    encoding: otlp_json

  kafka/logs-worklist:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-worklist"
    topic: otel-logs-worklist
    encoding: otlp_json

  kafka/logs-viewer:
    brokers: ["kafka:29092"]
    protocol_version: "2.6.0"
    client_id: "otel-collector-logs-viewer"
    topic: otel-logs-viewer
    encoding: otlp_json

service:
  extensions: [health_check]
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, batch]
      exporters: [debug, kafka/traces]

    metrics:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, batch]
      exporters: [debug, kafka/metrics]

    # Pipeline for OHIF (EventHub)
    logs/ohif:
      receivers: [azureeventhub/ohif]
      processors: [memory_limiter, resource/ohif, batch]
      exporters: [kafka/logs-ohif]

    # Pipeline for Dictation Backend (EventHub)
    logs/dictation_backend:
      receivers: [azureeventhub/dictation_backend]
      processors: [memory_limiter, resource/dictation_backend, batch]
      # Using shared dictation topic or could split further. Let's start with shared 'otel-logs-dictation'
      exporters: [kafka/logs-dictation_backend]

    # Pipeline for Dictation Frontend (EventHub)
    logs/dictation_frontend:
      receivers: [azureeventhub/dictation_frontend]
      processors: [memory_limiter, resource/dictation_frontend, batch]
      exporters: [kafka/logs-dictation_frontend]

    # Pipelines for Specific Domains (Filtered from OTLP)
    logs/auth-session:
      receivers: [otlp]
      processors:
        [
          memory_limiter,
          resourcedetection,
          transform,
          filter/auth-session,
          batch,
        ]
      exporters: [kafka/logs-auth-session]

    logs/worklist:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/worklist, batch]
      exporters: [kafka/logs-worklist]

    logs/viewer:
      receivers: [otlp]
      processors:
        [memory_limiter, resourcedetection, transform, filter/viewer, batch]
      exporters: [kafka/logs-viewer]

    # Pipeline for Default/Fallback Logs
    logs/default:
      receivers: [otlp]
      processors:
        [
          memory_limiter,
          resourcedetection,
          transform,
          filter/default_exclude,
          batch,
        ]
      exporters: [kafka/logs-default]
